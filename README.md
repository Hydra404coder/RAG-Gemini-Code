# ğŸš€ YTRAG - Retrieval-Augmented Generation System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.13+-3776ab?style=for-the-badge&logo=python&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-1.2.7+-00A3E0?style=for-the-badge)
![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector_DB-FF6B6B?style=for-the-badge)
![Google Generative AI](https://img.shields.io/badge/Google%20Gemini-2.5%20Flash-4285F4?style=for-the-badge&logo=google&logoColor=white)
![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-0096d6?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-00C851?style=for-the-badge)

**A Powerful Retrieval-Augmented Generation System for Document Intelligence & Question Answering**

[Features](#features) â€¢ [Architecture](#architecture) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Pipeline Workflow](#pipeline-workflow)

</div>

---

## ğŸ“‹ Overview

**YTRAG** is an enterprise-grade **Retrieval-Augmented Generation (RAG)** system that combines state-of-the-art document processing, semantic search, and generative AI to create intelligent question-answering systems. It enables you to ingest multiple document types (PDFs, text files, CSV), convert them to embeddings, store them securely in a vector database, and retrieve relevant context for precise AI-generated answers.

### ğŸ¯ Key Capabilities
- **Multi-format Document Support**: Process PDFs, Text files, and CSV data
- **Semantic Search**: Find relevant documents using embedding similarity
- **LLM Integration**: Powered by Google Gemini for intelligent responses  
- **Vector Storage**: Persistent ChromaDB for efficient retrieval
- **Advanced Retrieval**: Confidence scoring, source tracking, and history management
- **Streaming & Summarization**: Real-time responses and content summarization

---

## âœ¨ Features

### ğŸ“ Data Ingestion Pipeline
- âœ… **TextLoader**: Load individual text files with custom encoding
- âœ… **DirectoryLoader**: Batch load all files from a directory
- âœ… **PyPDFLoader/PyMuPDFLoader**: Extract text and metadata from PDFs
- âœ… **CSVLoader**: Process structured data in CSV format
- âœ… **Metadata Preservation**: Maintain source, page, author information

### ğŸ”¤ Document Processing
- âœ… **RecursiveCharacterTextSplitter**: Intelligent chunking with overlap
- âœ… **Configurable Chunk Sizes**: Customize for your use case (1000 chars default)
- âœ… **Context Preservation**: Overlapping chunks maintain semantic continuity
- âœ… **Smart Separators**: Hierarchical split strategy (\n\n â†’ \n â†’ space â†’ char)

### ğŸ§  Embedding Generation
- âœ… **SentenceTransformer**: Using `all-MiniLM-L6-v2` model
- âœ… **High-Dimensional Embeddings**: 384-dimensional vector representations
- âœ… **Batch Processing**: Efficient encoding with progress tracking
- âœ… **Semantic Understanding**: Capture meaning beyond keywords

### ğŸ—‚ï¸ Vector Storage & Retrieval
- âœ… **ChromaDB Integration**: Persistent vector database
- âœ… **Similarity Search**: Cosine distance-based retrieval
- âœ… **Metadata Indexing**: Filter and track document sources
- âœ… **Scalable Storage**: Handle thousands of documents efficiently

### ğŸ¤– LLM-Powered Generation
- âœ… **Google Gemini 2.5 Flash**: Fast, accurate responses
- âœ… **Prompt Engineering**: Optimized context-aware prompts
- âœ… **Temperature Control**: Adjustable response creativity (0-2)
- âœ… **Token Management**: Control output length (up to 1024 tokens)

### ğŸ” Advanced RAG Features
- âœ… **Dual Retrieval Modes**: Simple RAG + Advanced RAG with citations
- âœ… **Confidence Scoring**: Know how relevant your results are
- âœ… **Source Attribution**: Track which documents powered each answer
- âœ… **Session History**: Maintain conversation context
- âœ… **Streaming Output**: Real-time response generation
- âœ… **Summarization**: Auto-generate concise summaries

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER QUERY                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   1. QUERY EMBEDDING        â”‚
        â”‚  (SentenceTransformer)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  2. SIMILARITY SEARCH       â”‚
        â”‚   (ChromaDB/FAISS)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  3. CONTEXT RETRIEVAL & RANKING     â”‚
        â”‚   - Top-K Results                   â”‚
        â”‚   - Filter by Confidence            â”‚
        â”‚   - Prepare Sources                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  4. PROMPT CONSTRUCTION             â”‚
        â”‚   - Context Injection               â”‚
        â”‚   - Query Integration               â”‚
        â”‚   - Format Optimization             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  5. LLM GENERATION (Gemini)         â”‚
        â”‚   - Stream Response                 â”‚
        â”‚   - Temperature: 0.1 (Factual)      â”‚
        â”‚   - Max Tokens: 1024                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  6. OUTPUT ENRICHMENT               â”‚
        â”‚   - Add Confidence Score            â”‚
        â”‚   - Attach Sources                  â”‚
        â”‚   - Store History                   â”‚
        â”‚   - Optional Summarization          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   ANSWER + METADATA         â”‚
        â”‚   + Citations + Confidence  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Pipeline Workflow

### Phase 1: Data Ingestion & Processing

```
Multiple Sources
  â”œâ”€ PDF Files (PyMuPDFLoader)
  â”œâ”€ Text Files (TextLoader)
  â””â”€ CSV Files (CSVLoader)
         â”‚
         â–¼
  DirectoryLoader (Batch Processing)
         â”‚
         â–¼
  Langchain Documents (page_content + metadata)
         â”‚
         â–¼
  RecursiveCharacterTextSplitter
  â”œâ”€ Chunk Size: 1000 characters
  â”œâ”€ Overlap: 200 characters
  â””â”€ Hierarchical separators
         â”‚
         â–¼
  Document Chunks (Ready for Embedding)
```

### Phase 2: Embedding & Vector Storage

```
Text Chunks
     â”‚
     â–¼
SentenceTransformer (all-MiniLM-L6-v2)
     â”‚
     â”œâ”€ 384-dimensional vectors
     â”œâ”€ Semantic representation
     â””â”€ Ready for similarity search
     â”‚
     â–¼
ChromaDB Collection
     â”œâ”€ Store embeddings
     â”œâ”€ Store metadata
     â”œâ”€ Persist to disk
     â””â”€ Enable fast retrieval
```

### Phase 3: Query Processing & Retrieval

```
User Query
     â”‚
     â–¼
Embedding Generation (Same Model)
     â”‚
     â–¼
Vector Similarity Search (Cosine Distance)
     â”‚
     â”œâ”€ Top-K retrieval (configurable)
     â”œâ”€ Score threshold filtering
     â””â”€ Similarity scoring
     â”‚
     â–¼
Retrieved Documents + Rankings
     â”‚
     â”œâ”€ Document content
     â”œâ”€ Metadata (source, page, author)
     â”œâ”€ Confidence scores
     â””â”€ Source attribution
```

### Phase 4: Answer Generation

```
Retrieved Context
     â”‚
     â”œâ”€ Combine with user query
     â”œâ”€ Inject into prompt template
     â””â”€ Format for LLM
     â”‚
     â–¼
Google Gemini 2.5 Flash
     â”‚
     â”œâ”€ Temperature: 0.1 (Factual)
     â”œâ”€ Max tokens: 1024
     â””â”€ Stream output (optional)
     â”‚
     â–¼
Generated Answer + Sources + Confidence Score
```

---

## ğŸ› ï¸ Installation

### Prerequisites
- **Python 3.13+**
- **pip** or **uv** package manager
- **Google Generative AI API Key**

### Step 1: Clone & Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/YTRAG.git
cd YTRAG

# Create virtual environment
python -m venv venv
source venv/Scripts/activate  # On Windows
```

### Step 2: Install Dependencies

Using `pip`:
```bash
pip install -r requirements.txt
```

Or using `uv` (faster):
```bash
uv add -r requirements.txt
```

### Step 3: Configure API Keys

Create a `.env` file in the root directory:
```env
GOOGLE_API_KEY=your_google_generative_ai_api_key_here
```

Get your API key from: https://makersuite.google.com/app/apikey

---

## ğŸ“¦ Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `langchain` | 1.2.7+ | Core RAG framework |
| `langchain-core` | 1.2.7+ | Base components |
| `langchain-community` | 0.4.1+ | Document loaders |
| `langchain-google-genai` | 4.2.0+ | Gemini LLM integration |
| `sentence-transformers` | 5.2.2+ | Embedding generation |
| `chromadb` | 1.4.1+ | Vector database |
| `faiss-cpu` | 1.13.2+ | Vector similarity search |
| `pypdf` | 6.6.2+ | PDF processing |
| `pymupdf` | 1.26.7+ | Advanced PDF extraction |
| `python-dotenv` | Latest | Environment variables |
| `google-generativeai` | 0.8.6+ | Gemini API client |

---

## ğŸš€ Usage

### Basic Usage: Simple RAG

```python
from notebook.document import (
    TextLoader, DirectoryLoader, 
    RecursiveCharacterTextSplitter,
    EmbeddingManager, VectorStore, RAGRetriever,
    ChatGoogleGenerativeAI, rag_simple
)
import os
from dotenv import load_dotenv

# Load environment
load_dotenv()

# 1. Load documents
dir_loader = DirectoryLoader(
    "../data/text_files",
    glob="**/*.txt",
    loader_cls=TextLoader,
    loader_kwargs={'encoding': 'utf-8'}
)
documents = dir_loader.load()

# 2. Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# 3. Generate embeddings
embedding_manager = EmbeddingManager(model_name='all-MiniLM-L6-v2')
texts = [doc.page_content for doc in chunks]
embeddings = embedding_manager.generate_embeddings(texts)

# 4. Store in vector database
vector_store = VectorStore(collection_name="documents")
vector_store.add_documents(chunks, embeddings)

# 5. Create retriever
rag_retriever = RAGRetriever(vector_store, embedding_manager)

# 6. Initialize LLM
llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-flash-lite",
    temperature=0.1
)

# 7. Query with Simple RAG
answer = rag_simple("Your question here?", rag_retriever, llm, top_k=3)
print(answer)
```

### Advanced Usage: Enhanced RAG with Citations

```python
# Use the advanced RAG pipeline
result = rag_advanced(
    query="What are Industrial Designs?",
    retriever=rag_retriever,
    llm=llm,
    top_k=5,
    min_score=0.2,
    return_context=True
)

print("Answer:", result["answer"])
print("Confidence:", result["confidence"])
print("Sources:", result["sources"])
print("Context:", result["context"][:300])
```

### Enterprise Usage: Full Advanced Pipeline

```python
# Initialize advanced pipeline
adv_rag = AdvancedRAGPipeline(rag_retriever, llm)

# Query with streaming, summarization, and history
result = adv_rag.query(
    question="APPLE INC. VS. SAMSUNG case summary",
    top_k=5,
    min_score=0.1,
    stream=True,        # Stream response
    summarize=True      # Auto-summarize
)

print("Answer:", result["answer"])
print("Summary:", result["summary"])
print("History:", result["history"])
print("Sources:", result["sources"])
```

---

## ğŸ“Š Data Structure

### Document Metadata Example
```python
{
    "source": "file1.txt",
    "page": 1,
    "author": "Akhil Shibu",
    "date_created": "2026-02-02",
    "content_length": 1250,
    "doc_index": 0
}
```

### Retrieval Result Structure
```python
{
    "id": "doc_a1b2c3d4_0",
    "content": "Text chunk content...",
    "metadata": {...},  # See above
    "similarity_score": 0.87,
    "distance": 0.13,
    "rank": 1
}
```

### RAG Query Response
```python
{
    "answer": "Generated answer...",
    "sources": [
        {
            "source": "file1.txt",
            "page": 1,
            "score": 0.87,
            "preview": "Text preview..."
        }
    ],
    "confidence": 0.87,
    "summary": "Concise answer summary...",
    "history": [...]  # Previous queries
}
```

---

## ğŸ“‚ Project Structure

```
YTRAG/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ pyproject.toml          # Project configuration
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ .env                   # API keys (create this)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ text_files/        # Input text documents
â”‚   â”‚   â”œâ”€â”€ file1.txt
â”‚   â”‚   â””â”€â”€ file2.txt
â”‚   â”œâ”€â”€ pdf/               # Input PDF documents
â”‚   â”œâ”€â”€ Csv_files/         # Input CSV documents
â”‚   â”‚   â””â”€â”€ rag_langchain_dataset.csv
â”‚   â””â”€â”€ vector_store/      # ChromaDB persistence
â”‚       â””â”€â”€ chroma.sqlite3
â”‚
â””â”€â”€ notebook/
    â””â”€â”€ document.ipynb     # Main Jupyter notebook with all pipelines
```

---

## ğŸ”‘ Key Classes

### 1. **EmbeddingManager**
Handles embedding generation using SentenceTransformers
```python
embedding_manager = EmbeddingManager(model_name='all-MiniLM-L6-v2')
embeddings = embedding_manager.generate_embeddings(texts)
```

### 2. **VectorStore**
Manages document embeddings using ChromaDB
```python
vector_store = VectorStore(collection_name="documents", persist_directory="../data/vector_store")
vector_store.add_documents(documents, embeddings)
```

### 3. **RAGRetriever**
Retrieves relevant documents based on query similarity
```python
rag_retriever = RAGRetriever(vector_store, embedding_manager)
results = rag_retriever.retrieve(query, top_k=5, score_threshold=0.2)
```

### 4. **AdvancedRAGPipeline**
Enterprise-grade RAG with history, streaming, and summarization
```python
adv_rag = AdvancedRAGPipeline(rag_retriever, llm)
result = adv_rag.query(question, top_k=5, stream=True, summarize=True)
```

---

## âš™ï¸ Configuration

### Customize Chunk Splitting
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,    # Larger chunks for summary
    chunk_overlap=400   # More overlap for context
)
```

### Adjust Embedding Model
```python
embedding_manager = EmbeddingManager(
    model_name='all-mpnet-base-v2'  # Better but slower
)
```

### Fine-tune LLM
```python
llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-flash-lite",
    temperature=0.5,        # More creative
    max_output_tokens=2048  # Longer answers
)
```

### Retrieval Parameters
```python
# More results, higher threshold
results = rag_retriever.retrieve(
    query=question,
    top_k=10,              # Get top 10
    score_threshold=0.5    # Only very similar docs
)
```

---

## ğŸ§ª Testing & Examples

### Test Query 1: Simple Question
```python
answer = rag_simple(
    "What are the acts and laws governing Industrial Design?",
    rag_retriever, llm, top_k=3
)
```

### Test Query 2: Complex Case Analysis  
```python
result = rag_advanced(
    "APPLE INC. VS. SAMSUNG ELECTRONICS case summary",
    rag_retriever, llm, top_k=5, return_context=True
)
```

### Test Query 3: With History Tracking
```python
result = adv_rag.query(
    "Explain industrial design registration process",
    top_k=5, stream=True, summarize=True
)
print(f"Total queries in history: {len(result['history'])}")
```

---

## ğŸ“ How RAG Works

**Retrieval-Augmented Generation** combines information retrieval with text generation:

1. **Retrieval Phase**: Find the most relevant documents related to the user's query
2. **Augmentation Phase**: Use retrieved documents as context
3. **Generation Phase**: Feed context to LLM for accurate, source-backed answers

**Benefits:**
- âœ… **Reduces Hallucinations**: Answers grounded in documents
- âœ… **Up-to-date Answers**: Can reference latest documents
- âœ… **Private Data Support**: Works with proprietary documents
- âœ… **Source Attribution**: Know where answers come from
- âœ… **Cost Efficient**: Smaller LLMs can work with context

---

## ğŸ”® Future Enhancements

- [ ] Multi-language support (translate documents)
- [ ] Hybrid search (keyword + semantic)
- [ ] Fine-tuning with custom datasets
- [ ] GraphRAG for knowledge graph integration
- [ ] Real-time document indexing
- [ ] Streaming video/audio support
- [ ] Multi-turn conversation memory
- [ ] Custom prompt templates
- [ ] Performance monitoring & analytics
- [ ] Docker containerization

---

## ğŸ“ License

This project is open source. Feel free to use, modify, and distribute.

---

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“§ Contact & Support

- **Author**: Akhil Shibu
- **Email**: [akhilshibu2710@gmail.com]
- **Issues**: Report bugs via GitHub Issues
- **Discussions**: Start a discussion for feature requests

---

## ğŸŒŸ Acknowledgments

Built with:
- ğŸ¦œ [LangChain](https://python.langchain.com/)
- ğŸ¤– [Google Generative AI](https://ai.google.dev/)
- ğŸ” [ChromaDB](https://www.trychroma.com/)
- ğŸ“Š [Sentence Transformers](https://www.sbert.net/)
- ğŸ” [FAISS](https://github.com/facebookresearch/faiss)

---

<div align="center">

**Made with â¤ï¸ by Akhil Shibu**

[![Stars](https://img.shields.io/github/stars/yourusername/YTRAG?style=social)](https://github.com/yourusername/YTRAG)
[![Forks](https://img.shields.io/github/forks/yourusername/YTRAG?style=social)](https://github.com/yourusername/YTRAG)

</div>
